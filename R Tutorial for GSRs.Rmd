---
title: "R Tutorial for GSRs"
author: "Randy Clopton"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview of RStudio
This document was created using RStudio, an IDE designed to make R easier to use
and to add in additional functionality. RStudio is the place you should go to 
get started on your R journey. To use RStudio, you do need to download R
separately, so make sure you've downloaded both.

You can download R here: https://www.r-project.org/

You can download RStudio here: https://posit.co/products/open-source/rstudio/

In addition, to use some of the key functionality that RStudio has built in, it 
may help to download a TeX editor (this just lets you use some formatting
features of RStudio better, and is totally optional), You can download the
preferred version for RStudio here: https://miktex.org/download

## RStudio Layout
RStudio has four main panes.
* Source pane: where you see your code
* Console pane: where you see your code run
* Environment pane: shows you what objects you've created, plus the code you've
run recently
* Output pane: shows you any files, plots, or other things you've created. Also
lets you find new packages, use the help feature, and enter presentation mode.

## R Script and Markdown Files
All of your work will be done in one of these two file types. R Scripts are just
documents where you put all of your code. R Markdown documents are a bit fuller
featured and let you do things like text formatting, organizing your code into
discrete chunks, and printing your output directly in the source pane. The main
advantage of markdown files for students is to create well formatted documents 
that are ready to use as your finalized problem set.

## RStudio Projects
RStudio also has a projects feature that lets you tie together a number of 
different files that are intended to be used in concert, including code, data 
files, and anything you output as part of your process. Creating a project also
tells R to load a directory that you created to contain all your files, which 
makes it easier to call up different files.

## Loading Relevant Packages
Before you start anything up in R, you will need to load in any packages you
want to use to conduct your analysis. We will discuss useful packages later, 
but for now, here's what the process of loading in packages looks like

```{r Load Packages}
#tidyverse is probably the most useful package and at least one of its elements
#will probably be loaded in every project you do
library(tidyverse)
#the plm package is a go-to for panel data. Once you get to fixed effects, this
#package will let you easily run models with defined fixed effects.
library(plm)
```

# Data Import and Manipulation (Ripper)
On the very off chance your data are perfectly prepared and ready for analysis,
you will need to perform a form of initial loading and analysis to ensure
the data can work well in an R environment. This process starts with first
understanding the different data formats that are compatible with R and how
those formats can be used in different scenarios depending on project needs
and outcomes. We then focus on data types and structures to understand how R
processes your data and how we can best structure your data to work in your 
environment. We continue with showing how we can load your data in to R (with
examples to showcase how flexible R is with different data sources) and finish
by showing how R can shape and manipulate data to work well in further analysis.

## Data Formats and Use Cases
Even though R has the ability to load in data from different file types, there
exists two native data formats particular to use in an R environment. These
two formats are Rdata (Rda) and Rds. One would use these formats in order to
directly save R objects created in a session for later use. The Rdata data
format is used to save multiple R objects where the Rds data format can save
a single R object. Depending on use case, the user has the option to save
multiple or singular objects within a native R data format. The user even has
the option to save the entire work space (an entire workflow or analysis) for
later use.

### Data Types
There are five primary data types in R. They are listed below with quick
examples. We will also discuss functions that examine the features of R objects.

**Data Types**
1. Character: "p", "CRD"
2. Numeric: 5, 3.14
3. Integer: 3L (the L forces R to store as an integer)
4. Logical: TRUE (1), FALSE (0)
5. Complex: 2 + 3i

**Functions for Feature Examination**
1. class() - What kind of object is it?
2. typeof() - What is the object's data type?
3. length() - How long is it? What are its dimensions?
4. attributes() - Does it have any metadata?

Please be aware that some functionality in R forces the use of similar data
types in order to ensure consistency. For example, R will not let the user
add together a numeric object with a character data type. It is also important
to note that logicals can be treated as 1s or 0s depending on the context in
which they are used.

### Data Structures
We will discuss the most important data structures available in R. We will also
discuss exploratory functions to help us better understand the contents of
different data structures and how this will lead to a full data pipeline.

First, we observe the primary data structures in R:
1. Vector
2. List
3. Matrix
4. Data Frame

**Vector**
A vector is a collection of entries that are of the same data type. The
creation of the vector will allow for entries to be of different data
types but R will force the conversion of the other entries so that all entries
are of the same data type. One example would be creating a vector of the entries
1, 3, and "r". R will force all entries to be of a character data type since
it can convert 1 and 3 to a character but cannot convert "r" to a numeric or
integer data type. Below are some examples creating a vector, adding to a
vector, and examining the contents of the vector. Vectors can also handle
missing data entries (NA). We will examine how we can identify missing
entries as well.

```{r Vector}
# Create a vector.
v <- c(2, 3)

# Add an element to the vector.
v <- c(v, 5, 7)

# Observe the updated vector.
v

# Get the third element from the vector (NOTE: R is 1 indexed).
v[[3]]

# Examine the type of contents in the vector.
typeof(v)

# Examine the length of the vector.
length(v)

# Add an NA element to the vector.
v <- c(NA, v, NA)

# Check if the vector has any NA values.
anyNA(v)

# Check to see where the vector has a NA values.
which(is.na(v))
```

**List**
A list is similar to a vector but allows for entries of different data types.
We can even coerce a vector to become a list. Again, we explore the creation of
a list, adding to a list, and examining contents of the list. A list can hold
other more developed data types and structures, such as other lists, matrices,
and data frames.

```{r List}
# Create a list.
l <- list(3, "t", FALSE, 3 + 3i, NA)

# Observe the list.
l

# Examine the length of the list.
length(v)

# Get the second element from the list.
l[[2]]

# Convert vector to list.
v <- as.list(v)
```

**Matrix**
Matrices are vectors of two dimensions. They carry the same general
characteristic of a vector (all entries must be of the same data type). We can
easily convert a vector to a matrix by defining its dimensions (number of rows 
and number of columns). We show how to construct a matrix and how to access
certain elements in a matrix.

```{r Matrix}
# We first define a vector.
v <- 1:6

# We can now transform the vector into a matrix by changing the dimensions.
dim(v) <- c(2, 3)

# We now observe the matrix has 2 rows and 3 columns.
v

# We can access entries in a matrix by indexing the row and then the column.
v[2, 2]

# We can also create a matrix using the matrix constructor function.
m <- matrix(1:6, nrow = 2, ncol = 3)

# We observe the matrix is built identically to the one above.
m
```

**Data Frame**
Most of your analysis will involve the use of a data frame with its ability to
house data of different type and the ease with which R makes it to import data
to a data frame. Data frames are a specific type of list where every element
of this list has the same length. This allows for the presence of missing
data and uniformity across observations. We usually create a data frame when
loading in data from  an external document. However, we will show how to create
a data frame on the fly and functionality to observe the data frame and its
construction.

```{r Data Frame}
# We start by creating a data frame by hand.
df <- data.frame(id = letters[1:10], x = 1:10, y = 11:20)

# We observe the entire data frame above.
df

# We can observe the top of the data frame.
head(df)

# We observe the bottom of the data frame.
tail(df)

# We observe the dimensions of the data frame.
dim(df)

# We reference certain elements in the data frame by indexing [row, column].
df[5, 2]

# We pull out entire columns from the data frame.
df[["id"]]

# We pull out columns also by the $ operator in R.
df$id
```

## Importing Data Files
Importing data files is the first step of your analysis. One of the most
important aspects of importing is knowing the data's file type and also where
the file is located on your computer.

To avoid confusion over computer directories, you can set the working directory
of the current R-Markdown file by clicking "Session" -> "Set Working Directory".
If the files you would like to import as data are located in the same directory
as the current R or R-Markdown file, click "To Source File Location". Otherwise,
click "Choose Directory...". You also do not need to set the working directory
and can instead use the full file location when loading in data.

```{r Working Directory}
# Confirm your working directory.
getwd()

# Can also set working directory using setwd(dir = desired directory).
```

```{r Import RData File}
## Example loading in an RData file.

# Load RData file to R object. RData file directly loads content to R objects.
load(file = "Tutorial_Data/RData_ex.rda")

# Make sure the data were loaded properly.
dim(rdata)

# Check the top of the data frame to ensure it was loaded properly.
head(rdata)
```

```{r Import Excel File}
## Example loading in an Excel file.

# Include library to load both XLS and XLSX files.
library(readxl)

# Load Excel file to R object.
excel <- read_excel("Tutorial_Data/Excel_ex.xlsx")

# Make sure the data were loaded properly.
dim(excel)

# Check the top of the data frame to ensure it was loaded properly.
head(excel)
```

```{r Import CSV File}
## Example loading in a CSV file.

# Load CSV file in to R object.
csv <- read.csv(file = "Tutorial_Data/CDE_Graduation_Counts.csv") #RC: Since I've chosen a .csv data file to load in, I'm going ahead and loading it so I can play with it in the analysis section

# Make sure the data were loaded properly.
dim(csv)

# Check the top of the data frame to ensure it was loaded properly.
head(csv)
```

```{r Import Stata File}
## Example loading in a Stata file.

# Include library to load Stata file.
library(haven)

# Load Stata file in to R object.
stata <- read_dta(file = "Tutorial_Data/Stata_ex.dta")

# Make sure the data were loaded properly.
dim(stata)

# Check the top of the data frame to ensure it was loaded properly.
head(stata)
```

R also has the ability to web-scrape and communicate with Application
Programming Interfaces (APIs). Web-scraping is a technique used to collect
information or data from a website. This process is relient on the inherent
structure of a website (the actual HTML objects). APIs are ways to collect
information or data from a service. This service may be offered by a government
agency or a company. These APIs are sometimes key protected and only available
behind pay-walls. Others are publicly accessible but may be limited by the
number of times you may ask the API to submit/transmit information and data.
Both of these techniques of collecting data (webscraping and API connectivity)
can be reviewed in further detail by following the links below:

[Web-scraping](https://r4ds.hadley.nz/webscraping)

[Application Programming Interfaces (APIs)](https://info201.github.io/apis.html)

## Data Shaping and the Pipe Function
Data shaping along with the pipe function give us the ability to manipulate our
data so that it is presented in a manner that makes the most sense for our
analysis and also a manner of analysis itself. We can take our data and
transform how it displays to gain some points of interest that we might not have
been aware of beforehand.

Data shaping is the method by which we can make our data "wide" or "long" using
the "reshape" library. We make our data "long" by using the "melt" function
built in to R so that we can extend our data to understand exact relationships
between variables and values. We make our data "wide" by using the "cast"
function built in to R so that we can aggregate content in a data frame to
present statistical values for observations across a grouping. We can use "melt"
and "cast" interchangeably to go from a "wide" data frame to a "long" data frame
and back.

```{r Data Shaping - Long}
# Include the "reshape" library.
library(reshape)

# Let us start with a preliminary data frame.
df <- data.frame(id = 1:10, letter = letters[1:10], x = 1:10, y = 11:20)

# First, let's make the data frame "long" by the "letter" variable.
df_long <- melt(df, id = "letter")

# Observe the "long" data frame.
df_long
```

We see that we were able to extract the "letter"-"variable"-"value" assocations
across the entire data frame using the "melt" function.

```{r Data Shaping - Wide}
# Let us now make the "long" data frame "wide" or back to its original format.
df_wide <- cast(df_long, id = letter)

# Observe the "wide" data frame.
df_wide
```

We observe that we return the original data frame using the "cast" function
where we translate the variable-value relationship back using the same original
variable. We can use the the "cast" function further by creating statistical
results across a grouping, such as a mean value across a certain grouping of
observations.



```{r Pipe Function}

```

### Creating a dummy variable (RC Note: this is a specific thing that needs to be in this section as it's what is going to get us to the analysis part of the document)
Since R cannot directly handle categorical/word values for features in our data
sets, we must handle these values so that R can proceed with analysis. To do so,
we create "dummy" variables or new features in our data set that replace the
original feature. These dummy variables take on a value of 1 or 0 depending
on the original value of an observation. 1 (TRUE) being the observation took on
the value in that feature, 0 (FALSE) otherwise. These dummy variables must be
created for each feature-value pair. We continue with an example of how to
hand-code a dummy variable and an example of how to take advantage of written
functionality to prepare all dummy variables for us.

```{r Dummy Variable - Slow}
# Ensure we have loaded in the CSV file from above.
cde_grad_counts <- csv

# For this example, we will create a dummy variable for "County.Name".

# Get the distinct values in "County.Name" for which we will create dummies.
cde_grad_counts %>%
  select(County.Name) %>% # Get the "County.Name" column.
  unique() %>% # Observe the unique values.
  head() %>% # Only observe the top 5 entries.
  
# We observe the first value is "Alameda".
# We Will treat this as our base value to avoid multicollinearity issues.
# Thus, we proceed to the next value, "Alpine".

# We create the dummy variable for "County.Name.Alpine).
cde_grad_counts <- cde_grad_counts %>% # Save updated data frame.
  mutate(County.Name.Alpine = ifelse(County.Name == "Alpine", 1, 0)) # Create.

# Observe how we have created the dummy variable.
head(cde_grad_counts)
```

We observe we have created a new column related to whether or not the
observation has a "County.Name" of "Alpine" (1 for yes and 0 otherwise). We will
need to continue this process of creating new columns in our data set for the
remaining values.

Or, we can rely on built-in functionality available in R to construct the dummy
variables for us without requiring the tedious work of defining new columns for
each feature-value combination across our entire data set.

```{r Dummy Variable - Slow}
# Load in original data frame from CSV.
cde_grad_counts <- csv

# Import the "fastDummies" library to quickly make dummy variables.
library(fastDummies)

# Create the dummy variables for corresponding features.
cde_grad_counts <- fastDummies::dummy_cols(cde_grad_counts,
                                           c("Aggregate.Level", 
                                             "Count.Name", 
                                             "District.Name", 
                                             "School.Name", 
                                             "Reporting.Category"),
                                           remove_first_dummy = TRUE,
                                           remove_selected_columns = TRUE)

# Observe how we have created the dummy variables.
head(cde_grad_counts)
```

# Conducting Statistical Analyses Using R (Clopton)
This section will cover some basics of tasks and tools that are common at GSPP 
as part of the core quantitative methods class. We will be using a real data set
Randy created for the C2C project containing a list of the number of students
that graduated high school in a given academic year.

## Generating Summary Statistics
```{r Summary Stats}
#examine the data a bit
#let's look at the column names
colnames(cde_grad_counts)

#mean
mean(cde_grad_counts$One.Year.Graduate.Count)
# Why didn't that give me a number? There's some N/As in the data! 
# Let's add something to help us here
mean(cde_grad_counts$One.Year.Graduate.Count, na.rm=T)
# Ideally, you should clean up your N/A values before you do any analysis

#median
median(cde_grad_counts$One.Year.Graduate.Count, na.rm=T)

#standard deviation
sd(cde_grad_counts$One.Year.Graduate.Count, na.rm=T)

```

## T Tests and Correlations
Before we get too deep into this, let's clean the data a bit. The graduation
counts are all for school years, so they look like this
```{r Academic Year}
unique(cde_grad_counts$Academic.Year) #the unique() function shows the unique values for an object
```
That's not really how we want that to look, so let's modify that column a bit to
give us discrete years

```{r Change Academic Year to single years}
#the substr() function lets us just take a chunk of a string. Here, we want just
#the 6th through 9th characters. We're also coercing it to a number.
cde_grad_counts=cde_grad_counts %>%
  mutate(Year=as.numeric(substr(Academic.Year,6,9))) %>%
  relocate(Year,1) #mutate adds columns to the end of the data frame, so I'm
                   #putting it at the front

head(cde_grad_counts)
```
Now that we have a number rather than a character for Year, it will be easier to
do some testing with it. Let's see if there's a correlation between the total
number of white graduates over the years and the total number of Hispanic
graduates over the years.

```{r Correlation White/Hispanic}
#First, we need to massage the data into a better form for a correlation test
corr_data=cde_grad_counts %>%
  select(Year,Reporting.Category, One.Year.Graduate.Count) %>%
  group_by(Year,Reporting.Category) %>%
  summarise(Grad.Count=sum(One.Year.Graduate.Count, na.rm=T)) %>%
  pivot_wider(names_from = Reporting.Category, values_from = Grad.Count)

corr_data

#now that the data is in a format where we can compare Hispanic and White, let's
#correlate
cor(corr_data$`Hispanic/Latino`,corr_data$White)

```
Interesting! There's a strong negative correlation here. When we look at the
data, we can see why. Hispanic/Latino graduates have increased in number while
White graduates have reduced over the same period. Remember, correlation does
not imply causation, so this observation doesn't mean much unless we dig
further.

Now let's attempt a T-test. We can subset this data all the way down to the
school level, so let's see if we can compare graduation numbers across two
counties in the Bay Area of roughly similar size: Alameda and Santa Clara. Let's
subset our data at the school level, then compare

```{r T-test}
#subset to school level
t_data=cde_grad_counts %>%
  select(County.Name,School.Name,One.Year.Graduate.Count) %>% #grab just what we need
  group_by(County.Name,School.Name) %>% #group by both county and school
  summarize(Grad.Count=sum(One.Year.Graduate.Count, na.rm=T)) %>% #summarize at that grouping level
  filter(County.Name %in% c("Alameda","Santa Clara")) %>% #we only want these two counties, double equal for Boolean
  select(County.Name,Grad.Count) 


```

```{r T-test}
#subset to school level
t_data=cde_grad_counts %>%
  select(County.Name,School.Name,One.Year.Graduate.Count) %>% #grab just what we need
  group_by(County.Name,School.Name) %>% #group by both county and school
  summarize(Grad.Count=sum(One.Year.Graduate.Count, na.rm=T)) %>% #summarize at that grouping level
  filter(County.Name %in% c("Alameda","Santa Clara")) %>% #we only want these two counties. The %in% operator is a great way to set a conditional without having to go through tons of different individual conditional statements
  select(County.Name,Grad.Count) %>%
  pivot_wider(names_from = County.Name, values_from = Grad.Count) #we need this data to be in wide format to do a t-test

t_data #note that, since there are different numbers of schools in these counties, the dataframe has two lists

t.test(unlist(t_data$Alameda),unlist(t_data$`Santa Clara`)) #we solve that list problem with the unlist command


```
These means are definitely different!

<<<<<<< HEAD
Let's take a look now at some more detailed methods. What if we want to know if
overall graduation rates are increasing over the years? We can run a regression
with year as the independent variable and grad counts as the dependent variable.

Note: there's some funky stuff in this covariate set. Not all of the years have 
the same demographic information, so we're going to need to do a bit of cleaning
to get a usable dataset. (note: this particular dataset is the result of 
combining two different datasets with different information in them, and the 
second dataset's reporting categories don't disaggregate race and gender for 
whatever reason)

=======

## Regressions
```{r}
grad_race=cde_grad_counts %>%
  filter(Reporting.Category %in% c("Hispanic/Latino","American Indian/Alaskan Native", 
                      "Asian","Pacific Islander","Filipino","African American","White",
                      "Two or more races","Not reported")) 

grad_reg=lm(One.Year.Graduate.Count ~ Year, data=grad_race)

summary(grad_reg)
```
Looks like we're getting some pretty significant numbers there on a simple
univariate regression. Let's add in some covariates here. 
reason).

```{r}
grad_reg_race=lm(One.Year.Graduate.Count ~ Year+Reporting.Category, data=grad_race)

summary(grad_reg_race)
```
Now that we've pulled out race as a covariate, we can see that the actual growth
year-to-year is much smaller than before.

## F Tests
Now that we've tried a couple of different regressions on this data, we should
take a look and see if the batch of race covariates we added earlier is making 
a significant difference in the overall model. To do this, we do an F test.


```{r}
anova(grad_reg, grad_reg_race)
```
Yup, those race variables are definitely adding something.

## Interaction Terms and Difference-in-Difference
In addition to just the basic linear model formulas, you can also add in 
interacting terms when setting up regression models. There's a couple of methods
to do this depending on the number of terms you want to interact. To do this, 
we're going to run a difference-in-difference model. We're going to use OUSD
[Measure Y from 2020](https://ballotpedia.org/Oakland_Unified_School_District,_California,_Measure_Y,_Bond_Issue_(November_2020)) 
as our distinguishing event and divide Alameda County into OUSD and non-OUSD 
schools to get a reasonable counterfactual. Measure Y is an OUSD school funding
measure that focused on improving building safety and infrastructure. (How would
this lead to more graduation? Maybe safety is important? I'm not sure. This
document is more functional than actual statistics.)

```{r}
#first, we need to filter down to just Alameda County and create a variable for OUSD/not-OUSD
alameda=cde_grad_counts %>%
  filter(County.Name=="Alameda") %>% #filter out everything not in Alameda County
  mutate(OUSD=ifelse(District.Name=="Oakland Unified",1,0),  #create a variable indexing OUSD or not. ifelse is a good this to use in this context to get a binary variable that is needed for diff-in-diff, or you can use fastDummies
         Measure.Y=ifelse(Year>2020,1,0)) #Measure Y was implemented in 2021, so this is our event

#the "*" command lets us easily include both variables individually and their interaction
#If you just want the single interaction, use ":" instead
measure_y=lm(One.Year.Graduate.Count ~ OUSD*Measure.Y, data=alameda) 
summary(measure_y)

```
Looks like the implementation of Measure Y didn't noticeably change anything in
OUSD's graduation counts compared to the rest of the county.

## Visualizations
```{r}

```

## Additional Useful Packages

